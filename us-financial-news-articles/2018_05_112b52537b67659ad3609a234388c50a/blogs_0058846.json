{"organizations": [], "uuid": "c6d863469624b2b2f76249f54de0cdae456bc419", "thread": {"social": {"gplus": {"shares": 0}, "pinterest": {"shares": 0}, "vk": {"shares": 0}, "linkedin": {"shares": 0}, "facebook": {"likes": 0, "shares": 0, "comments": 0}, "stumbledupon": {"shares": 0}}, "site_full": "www.cnbc.com", "main_image": "https://fm.cnbc.com/applications/cnbc.com/resources/img/editorial/2017/02/17/104290111-pymetrics_product_1.1910x1000.jpg", "site_section": "http://www.cnbc.com/id/19746125/device/rss/rss.xml#", "section_title": "Top News and Analysis (pro)", "url": "https://www.cnbc.com/2018/05/30/silicon-valley-is-stumped-even-a-i-cannot-remove-bias-from-hiring.html", "country": "US", "domain_rank": 767, "title": "Silicon Valley is stumped, even A.I. cannot always remove bias from hiring", "performance_score": 0, "site": "cnbc.com", "participants_count": 0, "title_full": "", "spam_score": 0.0, "site_type": "blogs", "published": "2018-05-30T16:43:00.000+03:00", "replies_count": 0, "uuid": "c6d863469624b2b2f76249f54de0cdae456bc419"}, "author": "", "url": "https://www.cnbc.com/2018/05/30/silicon-valley-is-stumped-even-a-i-cannot-remove-bias-from-hiring.html", "ord_in_thread": 0, "title": "Silicon Valley is stumped, even A.I. cannot always remove bias from hiring", "locations": [], "entities": {"persons": [{"name": "andrew mcafee", "sentiment": "none"}], "locations": [{"name": "silicon valley", "sentiment": "none"}, {"name": "new york city", "sentiment": "none"}], "organizations": [{"name": "mins ago cnbc.com", "sentiment": "none"}, {"name": "google", "sentiment": "none"}, {"name": "sloan school of management", "sentiment": "none"}, {"name": "mit", "sentiment": "none"}, {"name": "microsoft", "sentiment": "none"}, {"name": "mcafee", "sentiment": "none"}, {"name": "facebook", "sentiment": "none"}, {"name": "digital economy", "sentiment": "none"}]}, "highlightText": "", "language": "english", "persons": [], "text": "Updated 9 Mins Ago CNBC.com\nAt a recent MIT event on the future of work in New York City for its high-achieving alumni network, Andrew McAfee, co-director of MIT's Initiative on the Digital Economy and a principal research scientist at the university's Sloan School of Management, said leaders are realizing that a lot of their human practices, human resources and human capital practices are simply outdated.\nMcAfee's view: \"If you want the bias out, get the algorithms in.\"\nSilicon Valley is investing in many start-ups selling the idea that they can solve the problem of human bias in job-hiring decisions with artificial intelligence. But a new class of independent algorithm auditing firms and public policy experts — with experience at some of the largest tech companies in the world and educations from elite institutions — say 'algorithmic bias' has already been proved to exist in other areas. As a result, the rapid uptake of AIs for hiring in the market has moved too fast, and with too little scrutiny, they say. Source: Pymetrics\nAlgorithms can help HR professionals make smart hiring decisions, but these algorithms can often be biased against minorities, said speakers on a panel at the MIT event. The biases creep in because human bias influenced the algorithm, and it's up to humans to notice the bias and fix it.\nTraditional résumé review leads to women and minorities being at a 50 percent to 67 percent disadvantage, according to start-up pymetrics , which attempts to go well beyond the résumé in assessing job applicants using neuroscience games and AI.\nCompanies using AI can reduce those figures dramatically, pymetrics said, as long as the input data is accurate and remains unbiased.\nThat's a big \"if.\" AI can work, 'as long as' the input data is accurate\nCathy O'Neil, who also spoke at the MIT future-of-work event, said the hiring algorithms now coming into the human resources field are a perfect test case for her skepticism about the tech utopian movement, and she uses these job algorithms often in presentations.\nO'Neil, an academically trained mathematician who studied and worked at UC Berkeley, Harvard and MIT — and left a job on Wall Street to join the Occupy Wall Street movement and write a book on the dangers of algorithms — often employs a thought experiment in talks she gives: Imagine what a machine-learning hiring algorithm trained on Fox News data would result in, even if reasonable choices were being made by the data science team. Then she points out that it doesn't have to be an outrageous example like Fox News, because there is no perfect workplace with perfect hiring policies, perfect raises and promotion methods, and a culture that welcomes all people equally. \"When we blithely train algorithms on historical data, to a large extent we are setting ourselves up to merely repeat the past. ... We'll need to do more, which means examining the bias embedded in the data.\" -Cathy O'Neil, author of \"Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy\"\n\"It's going to take some real thought,\" said Dr. Lori Kletzer, an economics professor at Colby College in an interview with CNBC. \"It's not just going to happen. And it's important to raise the questions now. ... The implications are societal, so we can't just leave it to the market, because the market only cares about the bottom line.\"\nTo start, the makeup of the tech industry creating the hiring algorithms isn't perfect. While Silicon Valley has a long history of encouraging immigrant entrepreneurs and bringing in foreign workers from around the world on skill visas , it has been criticized for diversity in terms of hiring from within the national population.\nA report from the federal government's Government Accountability Office released in November 2017 found that the technology industry is behind other sectors in diversity of its workforce. \"The estimated percentage of minority technology workers increased from 2005 to 2015, but GAO found that no growth occurred for female and black workers, whereas Asian and Hispanic workers made statistically significant increases. Further, female, black and Hispanic workers remain a smaller proportion of the technology workforce — mathematics, computing and engineering occupations — compared to their representation in the general workforce.\"\n\"When we blithely train algorithms on historical data, to a large extent we are setting ourselves up to merely repeat the past. If we want to get beyond that, beyond automating the status quo, we'll need to do more, which means examining the bias embedded in the data. The data is, after all, simply a reflection of our imperfect culture,\" O'Neil, who now runs her own algorithm auditing firm , said via email. The traditional job application process isn't working\nDr. Frida Polli, pymetrics CEO and co-founder, also has an extensive academic résumé, which includes an MBA from Harvard and a postdoctoral fellowship in neuroscience from MIT. Though despite the impressive accomplishments, she feels that simply listing them on her résumé didn't provide employers much information about her potential.\nPymetrics is working with companies such as Unilever , Accenture, LinkedIn and Tesla . The company uses behavioral neuroscience and artificial intelligence to help identify candidates in a more predictive and unbiased way. Pymetrics bypasses the résumé, using data generated from brain games to match applicants with roles. show chapters 2:07 PM ET Tue, 13 March 2018 | 04:40\nHireVue is another start-up working with the corporate industrial psychologists to make sure employer assessment tools are up to industry standards and, by adding AI to the mix, eliminating bias . It has been around for more than a decade, starting with tech that allowed for video interviews and moving more recently to AI-based candidate review.\n\"We can measure it, unlike the human mind, where we can't see what they're thinking or if they're systematically biased,\" Lindsey Zuloaga, director of data science at HireVue, recently told CNBC.\nOnce the candidate reaches a human recruiter, companies using HireVue have reported a much more diverse candidate pool: Unilever has improved the diversity of its talent pool by 16 percent since partnering with HireVue. \"If the team does notice a skew in results, it can evaluate the algorithm to see what went wrong and remove the bad data,\" Zuloaga said.\n\"AI is not impartial or neutral,\" said Meredith Whittaker, co-founder of the AI Now Institute at New York University, and founder of Google 's Open Research group. AI Now — which Whittaker co-founded with Kate Crawford, an NYU professor and principal researcher at Microsoft Research — aims to move beyond what it describe as \"minimal oversight\" of AI. Algorithmic bias is one of its core research areas.\n\"In the case of systems meant to automate candidate search and hiring, we need to ask ourselves: What assumptions about worth, ability and potential do these systems reflect and reproduce? Who was at the table when these assumptions were encoded?\" Whittaker asked.\nMore from @Work:", "external_links": ["https://www.pymetrics.com/employers/", "http://ide.mit.edu/", "https://www.gao.gov/assets/690/688459.pdf", "https://ainowinstitute.org/", "http://www.oneilrisk.com/"], "published": "2018-05-30T16:43:00.000+03:00", "crawled": "2018-05-30T16:52:34.000+03:00", "highlightTitle": ""}