{"organizations": [], "uuid": "dacd83db4bb6285a970265cb5e56f94402b97b49", "thread": {"social": {"gplus": {"shares": 0}, "pinterest": {"shares": 0}, "vk": {"shares": 0}, "linkedin": {"shares": 0}, "facebook": {"likes": 31, "shares": 31, "comments": 0}, "stumbledupon": {"shares": 0}}, "site_full": "blogs.wsj.com", "main_image": "", "site_section": "http://blogs.wsj.com/cio/feed/", "section_title": "CIO Journal.", "url": "https://blogs.wsj.com/cio/2018/05/02/companies-grapple-with-ais-opaque-decision-making-process/", "country": "US", "domain_rank": 387, "title": "Companies Grapple With AI’s Opaque Decision-Making Process", "performance_score": 0, "site": "wsj.com", "participants_count": 2, "title_full": "", "spam_score": 0.0, "site_type": "blogs", "published": "2018-05-02T22:15:00.000+03:00", "replies_count": 1, "uuid": "dacd83db4bb6285a970265cb5e56f94402b97b49"}, "author": "Sara Castellanos", "url": "https://blogs.wsj.com/cio/2018/05/02/companies-grapple-with-ais-opaque-decision-making-process/", "ord_in_thread": 0, "title": "Companies Grapple With AI’s Opaque Decision-Making Process", "locations": [], "entities": {"persons": [{"name": "zoubin ghahramani", "sentiment": "neutral"}, {"name": "sara castellanos", "sentiment": "none"}], "locations": [{"name": "new york", "sentiment": "none"}], "organizations": [{"name": "uber", "sentiment": "neutral"}, {"name": "intel corp.", "sentiment": "none"}, {"name": "o’reilly media inc.", "sentiment": "none"}, {"name": "uber inc.", "sentiment": "none"}, {"name": "wall street journal", "sentiment": "none"}, {"name": "xerox corp.", "sentiment": "none"}, {"name": "oreilly media inc.", "sentiment": "none"}]}, "highlightText": "", "language": "english", "persons": [], "text": "2 COMMENTS Zoubin Ghahramani, chief scientist at Uber, speaks at an AI conference this week hosted by OReilly Media Inc. and Intel Corp.s AI division. Photo: Sara Castellanos / The Wall Street Journal\nNEW YORK — Artificial intelligence is becoming more pervasive as companies look to drive innovation and competitive advantage, but some executives say they are coming up against a big challenge: advanced AI systems are not able to explain how they make decisions.\nExecutives at companies such as Uber Inc. and the research and development arm of Xerox Corp. said at a conference Tuesday that they’re investing in an area of AI research called “interpretability,” in an effort to understand exactly how complex AI systems solve problems.\n“It’s a really fruitful area of research and it’s been massively neglected for the whole history of machine learning,” said Zoubin Ghahramani, chief scientist at Uber, at an AI conference hosted by O’Reilly Media Inc. and Intel Corp.’s AI division.\nMachine learning enables computers to learn from data with minimal programming, and is a large part of artificial intelligence , a term that encompasses the techniques used to teach computers how to learn, reason, perceive, infer, communicate and make decisions like humans do.\nResearching interpretability is important for ethical, reputational and legal reasons in which it’s necessary to figure out how an automated system made a particular decision, Mr. Ghahramani said. But the importance of interpretability depends on the specific AI application, he said. Interpretability matters in the medical field, less so for captioning images, he said.\nWithin Uber, there is an active group looking at so-called AI neuroscience, which involves understanding the behavior and architecture of AI systems, he said.\n“It’s definitely important to us and something we’re paying attention to,” he said, adding that he’s personally interested in understanding how AI systems can explain their decisions by writing reports.\nUber is among several companies, private institutions and researchers interested in building a greater level of trust between humans and machines through transparency in artificial intelligence.\nThe research arm of the U.S. Department of Defense is coordinating an effort to build “explainable AI” systems that can translate complex algorithmic-made decisions into language humans can understand, for example.\nCapital One Financial Corp., too, is researching ways that machine-learning algorithms could explain the rationale behind their answers , which could have far-reaching impacts in guarding against potential ethical and regulatory breaches as the firm uses more artificial intelligence in banking.\nPARC, a research and development lab wholly owned by Xerox Corp., also has researchers devoted to transparency in AI, said Tolga Kurtoglu, CEO of PARC, at the AI conference.\nNot all problems require very detailed level of explainability, Mr. Kurtoglu said. But there are enough problems “where we absolutely need to build transparency into the AI systems, where we’d benefit as a society from the ability of those algorithms to articulate themselves.”\nAn AI system that can explain its decisions is helpful for two main reasons: when you have to decide whether to override the decision it’s making, and when you want to try to improve decisions it will make in the future, said Peter Norvig, director of research at Google, who also spoke at the conference.\nGoogle is developing tools to understand artificial neural networks and other types of machine learning algorithms, Mr. Norvig said in an email. Researchers at Google have visualized what a machine learning system is learning and the ability to debug a deep learning system, he said.\nShare this: ARTIFICIAL INTELLIGENCE EXPLAINABLE AI GOOGLE UBER XEROX PARC Previous Podcast: Blockchain as Lie Detector", "external_links": [], "published": "2018-05-02T22:15:00.000+03:00", "crawled": "2018-05-03T04:42:20.016+03:00", "highlightTitle": ""}